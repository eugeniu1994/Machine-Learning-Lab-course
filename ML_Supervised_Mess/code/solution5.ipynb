{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- option 2\n",
    "using the formula given in page 3, lecture 8, the correct answer is: 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability that the ensemble makes an incorrect prediction is: 0.05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "def binom(n, k):\n",
    "    #computes the binomial coefficient \n",
    "    return math.factorial(n) // math.factorial(k) // math.factorial(n - k)\n",
    "L=15\n",
    "eps=0.3\n",
    "sum_prob=0\n",
    "#sums up the probability of the cases where more than half of the base learers are wrong\n",
    "for k in range(math.ceil(L/2),L+1):\n",
    "    sum_prob+=(binom(L, k)*np.power(eps,k)*np.power(1-eps,L-k))\n",
    "print(\"Probability that the ensemble makes an incorrect prediction is:\",np.round(sum_prob,4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- option 2\n",
    "In AdaBoost algorithm page 20, lecture 8, in order to obtain positive weights(page 20, line 5) for a base learner, the corresponding weighted error should be less than 0.5 (random guessing).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- option 1\n",
    "\n",
    "AdaBoost minimizes exponential loss, whereas LogitBoost applies logistic loss as the surrogates for zero-one loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4- option 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.783\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x1 = np.array([.1,.2,.4,.8, .8, .05,.08,.12,.33,.55,.66,.77,.22,.2,.3,.6,.5,.6,.25,.3,.5,.7,.6])\n",
    "x2 = np.array([.2,.65,.7,.6, .3,.1,.4,.66,.22,.65,.68,.55,.44,.1,.3,.4,.3,.15,.15,.5,.55,.2,.4])\n",
    "N=np.shape(x1)\n",
    "labels = np.array([1,1,1,1,1,1,1,1,1,1,1,1,1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1])\n",
    "X = np.vstack((x1,x2,np.ones(N))).T\n",
    "def perceptron (X,labels,sample_weight):\n",
    "    #Inputs:\n",
    "    #X:  a 2d array, each row represents an example of the training set\n",
    "    #labels: vector of the examples labels\n",
    "    #sample_weight: vector of examples weights given by Adaboost\n",
    "    #Output:\n",
    "    #pred_labels: the label predicted for each example\n",
    "    d = np.shape(X)[1]\n",
    "    w = np.zeros(d)\n",
    "    i = 1\n",
    "    while(any([element<=0 for element in [labels[ind]*np.dot(w,x) for ind,x in enumerate(X)] ])): \n",
    "        #misclassified examples\n",
    "        mistakes = np.where([element<=0 for element in [labels[ind]*np.dot(w,x) for ind,x in enumerate(X)] ])[0]\n",
    "        pairs = zip(mistakes, sample_weight[mistakes]) \n",
    "        sorted_pairs = sorted(pairs, key=lambda t: t[1], reverse = True) \n",
    "        #use the misclassified example with maximum weight given by Adaboost\n",
    "        misclass = sorted_pairs[0][0]\n",
    "        #weight update\n",
    "        w = w + labels[misclass]*X[misclass]\n",
    "        #labels prediction\n",
    "        pred_labels = [1 if x>0 else -1 for x in [np.dot(w,x) for x in X]]\n",
    "        i +=1\n",
    "        if (i>201):\n",
    "            break\n",
    "    return pred_labels\n",
    "def AdaBoost(X,y, M):\n",
    "    N = len(y)\n",
    "    y_predict_list,  estimator_weight_list, sample_weight_list = [], [],[]\n",
    "    #Initialize the sample weights\n",
    "    sample_weight = np.ones(N) / N\n",
    "    sample_weight_list.append(sample_weight.copy()) \n",
    "    for m in range(M):   \n",
    "        #fit a classifier\n",
    "        y_predict = perceptron(X,y,sample_weight)\n",
    "        #misclassifications\n",
    "        incorrect = (y_predict != y)\n",
    "        #weighted error \n",
    "        estimator_error = np.average(incorrect, weights=sample_weight, axis=0)\n",
    "        #estimator weight\n",
    "        estimator_weight =   0.5*np.log((1. - estimator_error) / estimator_error)\n",
    "        #compute the sample weights\n",
    "        sample_weight *= np.exp(estimator_weight *-1*np.asarray(y_predict* y)  ) \n",
    "        #normlamize the sample weights to sum up to 1\n",
    "        sample_weight /= 2* np.power((estimator_error *(1- estimator_error)),0.5)\n",
    "        y_predict_list.append(y_predict.copy())\n",
    "        estimator_weight_list.append(estimator_weight.copy())\n",
    "        y_predict_mat = np.asarray(y_predict_list)\n",
    "        estimator_weight_mat = np.asarray(estimator_weight_list)\n",
    "        #compute the ensemble prediction\n",
    "        preds = (np.array([np.sign((y_predict_mat[:,point] * estimator_weight_mat).sum()) for point in range(N)]))\n",
    "        #compute the accuracy\n",
    "        accuracy = (preds == y).sum() / N\n",
    "    return accuracy\n",
    "accuracy = AdaBoost(X,labels, M=5)\n",
    "print(\"accuracy: \", np.round(accuracy,3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
